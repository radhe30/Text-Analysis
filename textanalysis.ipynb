{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BsYsWjNt8app",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f17e6abb-65a1-4813-bf6c-8349ddf2d24b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fake_useragent\n",
            "  Downloading fake_useragent-2.0.3-py3-none-any.whl.metadata (17 kB)\n",
            "Downloading fake_useragent-2.0.3-py3-none-any.whl (201 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/201.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.1/201.1 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: fake_useragent\n",
            "Successfully installed fake_useragent-2.0.3\n"
          ]
        }
      ],
      "source": [
        "!pip install fake_useragent\n",
        "import numpy as np\n",
        "import re\n",
        "import os\n",
        "import pandas as pd\n",
        "from nltk.tokenize import RegexpTokenizer , sent_tokenize\n",
        "from urllib.request import urlopen\n",
        "from bs4 import BeautifulSoup\n",
        "from fake_useragent import UserAgent\n",
        "import requests\n",
        "import urllib.request,sys,time ,requests\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stopWordsFile =     '/content/StopWords_Generic.txt'\n",
        "positiveWordsFile = '/content/positive-words.txt'\n",
        "nagitiveWordsFile = '/content/negative-words.txt'"
      ],
      "metadata": {
        "id": "ZoGd2UmNB5D1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "input = pd.read_excel(\"/content/Input.xlsx\")\n",
        "input\n",
        "\n",
        "def get_article_names(urls):\n",
        "  titles = []\n",
        "  for i in range (len(urls)):\n",
        "    title = urls[i]\n",
        "    title_clean = title[title.index( \"m/\" ) + 2 :-1]. replace('-' , ' ')\n",
        "    titles.append(title_clean)\n",
        "  return titles\n",
        "\n",
        "urls =input[\"URL\"]\n",
        "urlsTitleDF = get_article_names(urls)\n",
        "urlsTitleDF"
      ],
      "metadata": {
        "id": "3F32-qscCFsv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://insights.blackcoffer.com/how-people-diverted-to-telehealth-services-and-telemedicine\"\n",
        "\n",
        "page=requests.get(url , headers={\"User-Agent\": \"XY\"})\n",
        "soup = BeautifulSoup(page.text , 'html.parser')\n",
        "#get title\n",
        "title = soup . find(\"h1\",attrs = { 'class' : 'entry-title'}).get_text()\n",
        "\n",
        "#get article text\n",
        "text = soup . find(attrs = { 'class' : 'td-post-content'}).get_text()\n",
        "# break into lines and remove leading and trailing space on each\n",
        "lines = (line.strip() for line in text.splitlines())\n",
        "# break multi-headlines into a line each\n",
        "chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
        "# drop blank lines\n",
        "text = '\\n'.join(chunk for chunk in chunks if chunk)\n"
      ],
      "metadata": {
        "id": "NACkC0-xCIWY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Loading positive words\n",
        "with open(positiveWordsFile,'r') as posfile:\n",
        "    positivewords=posfile.read().lower()\n",
        "positiveWordList=positivewords.split('\\n')\n",
        "\n",
        "\n",
        "# Loading negative words\n",
        "with open(nagitiveWordsFile ,'r' ,  encoding=\"ISO-8859-1\") as negfile:\n",
        "    negativeword=negfile.read().lower()\n",
        "negativeWordList=negativeword.split('\\n')\n",
        "\n",
        "#Loading stop words dictionary for removing stop words\n",
        "\n",
        "with open(stopWordsFile ,'r') as stop_words:\n",
        "    stopWords = stop_words.read().lower()\n",
        "stopWordList = stopWords.split('\\n')\n",
        "stopWordList[-1:] = []\n",
        "\n",
        "\n",
        "\n",
        "display( positiveWordList[:6]  , negativeWordList[:6] , stopWordList[:6])\n",
        ""
      ],
      "metadata": {
        "id": "E_FuP-9dCmgf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "#tokenizeing module and filtering tokens using stop words list, removing punctuations\n",
        "def tokenizer(text):\n",
        "    text = text.lower()\n",
        "    tokenizer = RegexpTokenizer(r'\\w+')\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    filtered_words = list(filter(lambda token: token not in stopWordList, tokens))\n",
        "    return filtered_words\n",
        "\n",
        "def positive_score (text):\n",
        "  posword=0\n",
        "  tokenphrase = tokenizer(text)\n",
        "  for word in tokenphrase :\n",
        "    if word in positiveWordList:\n",
        "       posword+=1\n",
        "\n",
        "    retpos = posword\n",
        "    return retpos\n",
        "\n",
        "def negative_score (text):\n",
        "  negword=0\n",
        "  tokenphrase = tokenizer(text)\n",
        "  for word in tokenphrase :\n",
        "    if word in negativeWordList : negword +=1\n",
        "\n",
        "    retneg = negword\n",
        "    return retneg\n",
        "\n",
        "def polarity_score (positive_score , negative_score) :\n",
        "  return (positive_score - negative_score) / ((positive_score + negative_score) + 0.000001)\n",
        "\n",
        "def total_word_count(text):\n",
        "    tokens = tokenizer(text)\n",
        "    return len(tokens)\n",
        "\n",
        "def AverageSentenceLenght (text):\n",
        "  Wordcount = len(tokenizer (text))\n",
        "  SentenceCount = len (sent_tokenize(text))\n",
        "  if SentenceCount > 0 : Average_Sentence_Lenght = Wordcount / SentenceCount\n",
        "\n",
        "  avg = Average_Sentence_Lenght\n",
        "\n",
        "  return round(avg)\n",
        "\n",
        "\n",
        "# Counting complex words\n",
        "def complex_word_count(text):\n",
        "    tokens = tokenizer(text)\n",
        "    complexWord = 0\n",
        "\n",
        "    for word in tokens:\n",
        "        vowels=0\n",
        "        if word.endswith(('es','ed')):\n",
        "            pass\n",
        "        else:\n",
        "            for w in word:\n",
        "                if(w=='a' or w=='e' or w=='i' or w=='o' or w=='u'):\n",
        "                    vowels += 1\n",
        "            if(vowels > 2):\n",
        "                complexWord += 1\n",
        "    return complexWord\n",
        "\n",
        "def percentage_complex_word(text):\n",
        "    tokens = tokenizer(text)\n",
        "    complexWord = 0\n",
        "    complex_word_percentage = 0\n",
        "\n",
        "    for word in tokens:\n",
        "        vowels=0\n",
        "        if word.endswith(('es','ed')):\n",
        "            pass\n",
        "        else:\n",
        "            for w in word:\n",
        "                if(w=='a' or w=='e' or w=='i' or w=='o' or w=='u'):\n",
        "                    vowels += 1\n",
        "            if(vowels > 2):\n",
        "                complexWord += 1\n",
        "    if len(tokens) != 0:\n",
        "        complex_word_percentage = complexWord/len(tokens)\n",
        "\n",
        "    return complex_word_percentage\n",
        "\n",
        "def fog_index(averageSentenceLength, percentageComplexWord):\n",
        "    fogIndex = 0.4 * (averageSentenceLength + percentageComplexWord)\n",
        "    return fogIndex"
      ],
      "metadata": {
        "id": "T8T2nkHRCqvm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "URLS = input [\"URL\"]\n",
        "URLS"
      ],
      "metadata": {
        "id": "RrKHKS3DCva8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corps = []\n",
        "for url in URLS:\n",
        "  page = requests.get(url, headers={\"User-Agent\": \"XY\"})\n",
        "  soup = BeautifulSoup(page.text, 'html.parser')\n",
        "  #Check if the element exists before calling get_text()\n",
        "  title_element = soup.find(\"h1\", attrs={'class': 'entry-title'})\n",
        "  if title_element:\n",
        "    title = title_element.get_text()\n",
        "  else:\n",
        "    title = \"Title not found\"  # Or skip the URL\n",
        "  text_element = soup.find(attrs={'class': 'td-post-content'})\n",
        "  if text_element:\n",
        "    text = text_element.get_text()\n",
        "  else:\n",
        "    text = \"Content not found\" # Or skip the URL\n",
        "  lines = (line.strip() for line in text.splitlines())\n",
        "  chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
        "  text = '\\n'.join(chunk for chunk in chunks if chunk)\n",
        "  corps.append(text)"
      ],
      "metadata": {
        "id": "AVmITU0IC0EK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "# Download the 'punkt_tab' data\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "id": "MIBHeNdPF3IB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df = pd.DataFrame({'title':urlsTitleDF,'corps': corps})\n",
        "df[\"total word count\"] = df[\"corps\"] . apply (total_word_count)\n",
        "df[\"percentage_complex_word\"] = df[\"corps\"] . apply (percentage_complex_word)\n",
        "df[\"complex_word_count\"] = df[\"corps\"] . apply (complex_word_count)\n",
        "df[\"AverageSentenceLenght\"] = df[\"corps\"] . apply (AverageSentenceLenght)\n",
        "df[\"positive_score\"] = df[\"corps\"] . apply (positive_score)\n",
        "df[\"negative_score\"] = df[\"corps\"] . apply (negative_score)\n",
        "df[\"polarity_score\"] = np.vectorize(polarity_score)(df['positive_score'],df['negative_score'])\n",
        "\n",
        "df"
      ],
      "metadata": {
        "id": "6mDfdcmlC3tz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final = df.drop(\"corps\", axis=1)\n",
        "final"
      ],
      "metadata": {
        "id": "lXs611MiC6bW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final.to_excel('Output Data Structure.xlsx')"
      ],
      "metadata": {
        "id": "sJxf9cSCF8mK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UPYu0lgBGHuh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}